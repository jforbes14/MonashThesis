# Methodology {#ch:Methodology}

Now that estimated socio-demographic profiles have been obtained for each election, we consider how to answer our research question, "What socio-demographics are the key determinants of support between the Labor and Liberal parties, and how have their effects changed over time?". 

## Response variable - two-party preferred vote

To answer this question, we set out to model the socio-demographics of electorates that are important in determining the two-party preferred vote, $TPP$. Since the two-party preferred vote is a measure of preference between only the two major parties (Labor and Liberal), we can focus our models on the response in favour of the Liberal party, which sits in the interval $(0,1)$.

Since the values that $TPP$ assumes are not close to 0 or 1 (minimum $0.24$ and maximum $0.75$), we do not need to worry about modelling the constraint of $TPP \in (0,1)$. Instead we can model the response using linear regression.

Therefore, for a given election year, we assume ${\bf TPP} \sim N({\bf X \beta}, \sigma^2 {\bf I})$, where $\bf TPP$ is an $n \times 1$ vector of two-party preferred vote, $\bf X$ is an ($n \times p$) matrix of socio-demographic predictors, $\bf \beta$ is a ($p \times 1$) vector of coefficients to be estimated, and errors are independent and identically distributed with mean $0$ and variance $\sigma^2$. We estimate $\bf \beta$ and $\sigma^2$ using ordinary least-squares regression, and conduct inference on the output. This estimation is done separately for each election.

It is important to note here that a longitudinal approach would be *less* appropriate than modelling each election separately. Electorate boundaries change regularly, so it would be very difficult to construct a data set that tracks the same spatial units over time - not just because of their socio-demographics, but also because of their voting behaviour.

Inference can then be conducted on the output, in order to determine which factors are statistically significant, and their marginal effects on the two party preferred vote will be computed.

## Dealing with the small $n$, large $p$ problem

We have $p = 63$ socio-demographic variables, and $n = 150$ electorates, at our disposal for each election. If we tried to fit a model for a single election using all 63 varibles, we face problems with multi-collinearity and over-fitting, and would lead to erroneous conclusions of variable significance. This presents us with a problem of having small $n$ and relatively large $p$. Therefore a form of dimension reduction is needed to build adequate models. 

Instead of throwing everything into a model, and using some kind of procedure to reduce the dimension of the data, we decide to adopt a two-stage approach in obtaining a reduced predictor set, from which we will run our final election models. The first step is to identify variables that are similar, and group them together into *factors*. Principal component analysis (PCA) is used to identify variables that are correlated, and we combine variables into a factor if we can reasonably believe that they should be grouped. This reduces our predictor set from $p = 50$ to $p = 30$, from which we still need to do some kind of variable selection, so that we end up with a set of around $10$ variables from which we can model each election.

The second step is an approach that will allow us to identify a handful of the *most important* variables from each election year. By taking the union of the six resultant variable subsets, a superset is created from which all election models will be re-fit. We therefore seek to estimate a good model that captures the more prominent relationships that exist between socio-demographics and electorate voting behaviour, based on the empirical data in hand. 

Our chosen method is an information-theoretic approach, which uses *Akaike weights* to measure relative variable importance [@BurnhamAnderson2002]. 

## Factor creation

Principal component analysis produces a low-dimensional representation of the socio-demographic data set, by finding a sequence of linear combinations that have maximal variance, and are mutually uncorrelated. The components are ordered by their proportion of variance explained. Although the components are not entirely interpretable, we can infer the relative importance of variables in each component based on the magnitude of their loadings.

The components for each election year are computed separately and then compared to determine their similarity. If the resultant components are sufficiently similar, we are justified in combining the six data sets, and then re-computing principal component analysis this combined set. We assess similarity by observing whether higher loadings in each component are reasonably similar across elections.

These components are then interpreted to extract what we denote as factors - groupings of variables that have loadings clustered together and have an intuitive interpretation.

## Superset creation

Even after reducing the dimension of our data set, we still have 30 possible predictors to choose from, and only 150 observations. To estimate a *good* model, we use variable selection to further reduce our data to 10 variables, which capture the five *most important* variables in determining $TPP$ in each election. Our approach is based off a technique common in ecological studies, using Akaike weights to measure relative variable importance [@BurnhamAnderson2002].

Fundamentally driven by Akaike's information criterion (AIC) [@Akaike73], which is a measure of expective information lost in estimating the true underlying data generating process [@BurnhamAnderson2002]. Minimizing AIC is our in determining best fit - should we be comparing multiple models in a given set $M$.

Let $\Delta_m = AIC_m - AIC_{min}$, which is the difference in AIC between model $m$ and the minimum AIC in the model set $m = 1,2,...,M$. *Akaike weights* $w_m$ for each model $m$ are then calculated:

$$w_m = \frac {\exp(-\frac{1}{2}\Delta_m)} {\sum_{r=1} ^R \exp(-\frac{1}{2}\Delta_r)}$$

The Akaike weight represents the posterior probability that model $m$ is the best model in the set, without any beliefs a priori. $\sum_{m=1}^{M} w_m$ and $w_m \in (0,1)$.

For each variable $j$, we compute the sum of Akaike weights ($s_j$) over models that include that variable, and use these as a measure of variable importance [@BurnhamAnderson2002]. Let $I$ denote the indicator function.

$$s_j = \sum_{j=1}^{J} \sum_{m=1}^{M} w_m \cdot I(j \text{ used in model } m)$$

We select the five variables with largest $s_j$ across all $j \in J$ in each election. We then construct a super-set by taking the union these five variables from each election, and use this super-set as our chosen predictors to be used across *all* elections in the final model. 

## Adding interactions

Two-way variable interactions are then added to the model using an iterative procedure. First we fit a model using the super-set of predictors as main effects only. Then, for each election, we compute the number of elections in which each possible two-way interaction significantly adds to the model fit, using a likelihood-ratio test (1% significance level). We then select the interaction that is significant in the most years and re-fit models by adding this interaction to the previous model specification. 

This is repeated until four interactions have been included. We limit this to four so that the number of observations (150) to predictors (14) exceeds a ratio of 10 to 1 (*note that in 2001, $n = 141$*).

Final models are then fit using the super-set of 10 main effects and the four two-way interactions.

## Inference

We conduct inference on the resultant models using tools from the `visreg` package. Main effects are interpreted by visualizing the partial residuals with a prediction line and confidence band. The slope of prediction line is the estimated coefficient of that main effect. Interactions are interpreted using a raster plot, which shows the predicted $TPP$ for combinations of the interacting variables on continuous scales.

